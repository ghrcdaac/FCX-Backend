{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "netcdf to zarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def add24hr(hr):\n",
    "    \"\"\"Correction of time in CRS for going over the next day in UTC\"\"\"\n",
    "    b = np.where(hr < hr[0])\n",
    "    hr[b] = hr[b] + 24\n",
    "    return hr\n",
    "\n",
    "def down_vector(roll, pitch, head):\n",
    "    x = np.sin(roll) * np.cos(head) + np.cos(roll) * np.sin(pitch) * np.sin(head)\n",
    "    y = -np.sin(roll) * np.sin(head) + np.cos(roll) * np.sin(pitch) * np.cos(head)\n",
    "    z = -np.cos(roll) * np.cos(pitch)\n",
    "    return (x, y, z)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zarr\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "# META needed for ingest\n",
    "campaign = 'Olympex'\n",
    "collection = \"AirborneRadar\"\n",
    "dataset = \"gpmValidationOlympexcrs\"\n",
    "variables = [\"zku\"]\n",
    "renderers = [\"point_cloud\"]\n",
    "chunk = 262144\n",
    "to_rad = np.pi / 180\n",
    "to_deg = 180 / np.pi\n",
    "\n",
    "def ingest(folder, file, s3bucket):\n",
    "    \"\"\"\n",
    "    Converts Level 1B crs data from s3 to zarr file and then stores it in the provided folder\n",
    "    Args:\n",
    "        folder (string): name to hold the raw files.\n",
    "        file (string): the s3 url to the raw file.\n",
    "    \"\"\"\n",
    "    store = zarr.DirectoryStore(folder)\n",
    "    root = zarr.group(store=store)\n",
    "    \n",
    "    # Create empty rows for modified data    \n",
    "    z_chunk_id = root.create_dataset('chunk_id', shape=(0, 2), chunks=None, dtype=np.int64)\n",
    "    z_location = root.create_dataset('location', shape=(0, 3), chunks=(chunk, None), dtype=np.float32)\n",
    "    z_time = root.create_dataset('time', shape=(0), chunks=(chunk), dtype=np.int32)\n",
    "    z_vars = root.create_group('value')\n",
    "    z_ref = z_vars.create_dataset('ref', shape=(0), chunks=(chunk), dtype=np.float32)\n",
    "    n_time = np.array([], dtype=np.int64)\n",
    "    file = \"olympex_CRS_20151210_205900-20151210_211038_2_v01a.nc\"\n",
    "    date = file.split(\"_\")[2]\n",
    "    # date = \"\"\n",
    "    # base_time = \"\"\n",
    "    base_time = np.datetime64('{}-{}-{}'.format(date[:4], date[4:6], date[6:]))\n",
    "\n",
    "    # open dataset.\n",
    "    with xr.open_dataset(\"../../test_data/crs/olympex_CRS_20151210_205900-20151210_211038_2_v01a.nc\", decode_cf=False) as ds:\n",
    "        # added for time correction for over 24h UTC\n",
    "        hr = add24hr(ds['timed'].values)\n",
    "        delta = (hr * 3600).astype('timedelta64[s]') + base_time\n",
    "        # time correction end\n",
    "        \n",
    "        # data columns extract\n",
    "        ref = ds[variables[0]].values #CRS radar reflectivity\n",
    "        lat = ds['lat'].values\n",
    "        lon = ds['lon'].values\n",
    "        alt = ds['altitude'].values # altitude of aircraft in meters\n",
    "        roll = ds[\"roll\"].values\n",
    "        pitch = ds[\"pitch\"].values\n",
    "        head = ds[\"head\"].values\n",
    "        rad_range = ds[\"range\"].values\n",
    "    num_col = ref.shape[0] # number of cols\n",
    "    num_row = ref.shape[1] # number of rows\n",
    "\n",
    "    # data frame formation\n",
    "    delta = np.repeat(delta, num_row)\n",
    "    lon = np.repeat(lon, num_row)\n",
    "    lat = np.repeat(lat, num_row)\n",
    "    alt = np.repeat(alt, num_row)\n",
    "    roll = np.repeat(roll * to_rad, num_row)\n",
    "    pitch = np.repeat(pitch * to_rad, num_row)\n",
    "    head = np.repeat(head * to_rad, num_row)\n",
    "    rad_range = np.tile(rad_range, num_col)\n",
    "    ref = ref.flatten()\n",
    "\n",
    "    # time correction.\n",
    "    time = (delta - np.datetime64('1970-01-01')).astype('timedelta64[s]').astype(np.int64)\n",
    "\n",
    "    # x, y, z = down_vector(roll, pitch, head)\n",
    "    # x = np.multiply(x, np.divide(rad_range, 111000 * np.cos(lat * to_rad)))\n",
    "    # y = np.multiply(y, np.divide(rad_range, 111000))\n",
    "    # z = np.multiply(z, rad_range)\n",
    "\n",
    "    # lon = np.add(-x, lon)\n",
    "    # lat = np.add(-y, lat)\n",
    "    # alt = np.add(z, alt)\n",
    "\n",
    "    # sort data by time\n",
    "    sort_idx = np.argsort(time)\n",
    "\n",
    "    lon = lon[sort_idx]\n",
    "    lat = lat[sort_idx]\n",
    "    alt = alt[sort_idx]\n",
    "    ref = ref[sort_idx]\n",
    "    time = time[sort_idx]\n",
    "\n",
    "    # remove nan and infinite using mask ???\n",
    "    mask = np.logical_and(np.isfinite(ref), alt > 0)\n",
    "    lon = lon[mask]\n",
    "    lat = lat[mask]\n",
    "    alt = alt[mask]\n",
    "    ref = ref[mask]\n",
    "    time = time[mask]\n",
    "\n",
    "    # Now populate (append) the empty rows with modified data.\n",
    "    z_location.append(np.stack([lon, lat, alt], axis=-1))\n",
    "    z_ref.append(ref)\n",
    "    n_time = np.append(n_time, time)\n",
    "\n",
    "    idx = np.arange(0, n_time.size, chunk)\n",
    "    chunks = np.zeros(shape=(idx.size, 2), dtype=np.int64)\n",
    "    chunks[:, 0] = idx\n",
    "    chunks[:, 1] = n_time[idx]\n",
    "    z_chunk_id.append(chunks)\n",
    "\n",
    "    epoch = np.min(n_time)\n",
    "    n_time = (n_time - epoch).astype(np.int32)\n",
    "    z_time.append(n_time)\n",
    "\n",
    "    # save it.\n",
    "    root.attrs.put({\n",
    "        \"campaign\": campaign,\n",
    "        \"collection\": collection,\n",
    "        \"dataset\": dataset,\n",
    "        \"variables\": variables,\n",
    "        \"renderers\": renderers,\n",
    "        \"epoch\": int(epoch)\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with xr.open_dataset(\"../../test_data/crs/olympex_CRS_20151210_205900-20151210_211038_2_v01a.nc\", decode_cf=False) as ds:\n",
    "    # added for time correction for over 24h UTC\n",
    "    hr = add24hr(ds['timed'].values)\n",
    "    delta = (hr * 3600).astype('timedelta64[s]') + base_time\n",
    "    # time correction end\n",
    "    \n",
    "    # data columns extract\n",
    "    ref = ds[variables[0]].values #CRS radar reflectivity\n",
    "    lat = ds['lat'].values\n",
    "    lon = ds['lon'].values\n",
    "    alt = ds['altitude'].values # altitude of aircraft in meters\n",
    "    roll = ds[\"roll\"].values\n",
    "    pitch = ds[\"pitch\"].values\n",
    "    head = ds[\"head\"].values\n",
    "    rad_range = ds[\"range\"].values\n",
    "    \n",
    "print(rad_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "zarr_folder = \"./tmp/exp/zarr/dropsonde\"\n",
    "ingest(zarr_folder, \"\", \"\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate point cloud\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from threading import Thread, Lock\n",
    "\n",
    "to_rad = np.pi / 180.0\n",
    "to_deg = 180.0 / np.pi\n",
    "\n",
    "steps = [32, 16, 8, 4, 2, 1]\n",
    "\n",
    "class PointCloud:\n",
    "    def __init__(self, key, lon, lat, alt, value, time, epoch):\n",
    "        self.key = key\n",
    "        self.lon = lon\n",
    "        self.lat = lat\n",
    "        self.alt = alt\n",
    "        self.time = time\n",
    "        self.value = value\n",
    "        self.epoch = epoch\n",
    "        self.tasks = []\n",
    "        self.threads = []\n",
    "        for i in range(10):\n",
    "            self.threads.append(Thread(target=self.worker_function))\n",
    "        self.tileset_lock = Lock()\n",
    "        self.tileset_json = {\n",
    "        \t\"asset\": {\n",
    "        \t\t\"version\": \"1.0\",\n",
    "        \t\t\"type\": \"Airborne Radar\"\n",
    "        \t},\n",
    "        \t\"root\": {\n",
    "        \t\t\"geometricError\": 1000000,\n",
    "        \t\t\"refine\" : \"REPLACE\",\n",
    "        \t\t\"boundingVolume\": {\n",
    "                    \"region\": [\n",
    "                        float(np.min(lon)) * to_rad,\n",
    "                        float(np.min(lat)) * to_rad,\n",
    "                        float(np.max(lon)) * to_rad,\n",
    "                        float(np.max(lat)) * to_rad,\n",
    "                        float(np.min(alt)) * to_rad,\n",
    "                        float(np.max(alt)) * to_rad\n",
    "                    ]\n",
    "                },\n",
    "                \"children\": []\n",
    "        \t},\n",
    "            \"properties\": {\n",
    "                \"epoch\": \"{}Z\".format(datetime.utcfromtimestamp(epoch).isoformat()),\n",
    "                \"refined\": []\n",
    "            }\n",
    "        }\n",
    "\n",
    "\n",
    "    def worker_function(self):\n",
    "        while len(self.tasks) > 0:\n",
    "                tile, start, end = self.tasks.pop()\n",
    "                print(tile, start, end)\n",
    "                self.generate(tile, start, end)\n",
    "\n",
    "\n",
    "    def start(self):\n",
    "        for t in self.threads:\n",
    "            t.start()\n",
    "\n",
    "\n",
    "    def join(self):\n",
    "        for t in self.threads:\n",
    "            t.join()\n",
    "        with open('{}/tileset.json'.format(self.key), mode='w+') as outfile:\n",
    "            json.dump(self.tileset_json, outfile)\n",
    "\n",
    "\n",
    "    def schedule_task(self, tile, start, end):\n",
    "        self.tasks.append((tile, start, end))\n",
    "\n",
    "\n",
    "    def generate(self, tile, start, end):\n",
    "        print(tile, start, end)\n",
    "        parent_tile = self.tileset_json[\"root\"]\n",
    "        cartesian, offset, scale, cartographic, region = self.cartographic_to_cartesian(start, end)\n",
    "\n",
    "        value = self.value[start:end]\n",
    "        time = self.time[start:end]\n",
    "\n",
    "        epoch = int(np.min(time) + self.epoch - 300)\n",
    "        epoch = \"{}Z\".format(datetime.utcfromtimestamp(epoch).isoformat())\n",
    "        end = int(np.max(time) + self.epoch + 300)\n",
    "        end = \"{}Z\".format(datetime.utcfromtimestamp(end).isoformat())\n",
    "\n",
    "        header_length = 28\n",
    "        magic = np.string_(\"pnts\")\n",
    "        version = 1\n",
    "\n",
    "        for step in steps:\n",
    "            self.tileset_lock.acquire()\n",
    "            try:\n",
    "                filename = \"{}_{}.pnts\".format(tile, step)\n",
    "                child_tile = {\n",
    "                    \"availability\": \"{}/{}\".format(epoch, end),\n",
    "                    \"geometricError\": step * 500,\n",
    "                    \"boundingVolume\": {\n",
    "                        \"region\": region\n",
    "                    },\n",
    "                    \"content\": {\n",
    "                        \"uri\": filename\n",
    "                    },\n",
    "                    \"refine\": \"REPLACE\"\n",
    "                }\n",
    "                if step == 1:\n",
    "                    self.tileset_json[\"properties\"][\"refined\"].append(filename)\n",
    "                else:\n",
    "                    child_tile[\"children\"] = []\n",
    "                parent_tile[\"children\"].append(child_tile)\n",
    "                parent_tile = child_tile\n",
    "            finally:\n",
    "                self.tileset_lock.release()\n",
    "\n",
    "            tile_length = 0\n",
    "            feature_table_binary_byte_length = 0\n",
    "            batch_table_binary_byte_length = 0\n",
    "            length = value[::step].size\n",
    "\n",
    "            feature_table_json = {\n",
    "                \"POINTS_LENGTH\": length,\n",
    "                \"BATCH_LENGTH\": length,\n",
    "                \"BATCH_ID\": {\n",
    "                    \"byteOffset\": 0,\n",
    "                    \"componentType\": \"UNSIGNED_INT\"\n",
    "                },\n",
    "                \"POSITION_QUANTIZED\": {\n",
    "                    \"byteOffset\": length * 4\n",
    "                },\n",
    "                \"QUANTIZED_VOLUME_OFFSET\": offset,\n",
    "                \"QUANTIZED_VOLUME_SCALE\": scale\n",
    "            }\n",
    "\n",
    "            batch_table_json = {\n",
    "                \"value\": {\n",
    "                    \"byteOffset\": 0,\n",
    "                    \"componentType\": \"FLOAT\",\n",
    "                    \"type\": \"SCALAR\"\n",
    "                },\n",
    "                \"time\": {\n",
    "                    \"byteOffset\": length * 4,\n",
    "                    \"componentType\": \"FLOAT\",\n",
    "                    \"type\": \"SCALAR\"\n",
    "                },\n",
    "                \"location\": {\n",
    "                    \"byteOffset\": length * 8,\n",
    "                    \"componentType\": \"SHORT\",\n",
    "                    \"type\": \"VEC3\"\n",
    "                }\n",
    "            }\n",
    "\n",
    "            tile_length += header_length\n",
    "\n",
    "            feature_table_json_min = json.dumps(feature_table_json, separators=(\",\", \":\")) + \"       \"\n",
    "            feature_table_trim = (tile_length + len(feature_table_json_min)) % 8\n",
    "            if feature_table_trim != 0:\n",
    "                feature_table_json_min = feature_table_json_min[:-feature_table_trim]\n",
    "\n",
    "            tile_length += len(feature_table_json_min)\n",
    "\n",
    "            feature_table_binary_byte_length = length * 4 + length * 3 * 2\n",
    "            tile_length += feature_table_binary_byte_length\n",
    "            feature_table_padding = tile_length % 8\n",
    "            if feature_table_padding != 0:\n",
    "                feature_table_padding = 8 - feature_table_padding\n",
    "            tile_length += feature_table_padding\n",
    "\n",
    "            batch_table_json_min = json.dumps(batch_table_json, separators=(\",\", \":\")) + \"       \"\n",
    "            batch_table_trim = (tile_length + len(batch_table_json_min)) % 8\n",
    "            if batch_table_trim != 0:\n",
    "                batch_table_json_min = batch_table_json_min[:-batch_table_trim]\n",
    "\n",
    "            tile_length += len(batch_table_json_min)\n",
    "\n",
    "            batch_table_binary_byte_length = length * 4 * 2 + length * 2 * 3\n",
    "            tile_length += batch_table_binary_byte_length\n",
    "            batch_table_padding = tile_length % 8\n",
    "            if batch_table_padding != 0:\n",
    "                batch_table_padding = 8 - batch_table_padding\n",
    "            tile_length += batch_table_padding\n",
    "\n",
    "            with open('{}/{}'.format(self.key, filename), mode='wb+') as outfile:\n",
    "                outfile.write(np.string_(magic).tobytes())\n",
    "                outfile.write(np.uint32(version).tobytes())\n",
    "                outfile.write(np.uint32(tile_length).tobytes())\n",
    "                outfile.write(np.uint32(len(feature_table_json_min)).tobytes())\n",
    "                outfile.write(np.uint32(feature_table_binary_byte_length + feature_table_padding).tobytes())\n",
    "                outfile.write(np.uint32(len(batch_table_json_min)).tobytes())\n",
    "                outfile.write(np.uint32(batch_table_binary_byte_length + batch_table_padding).tobytes())\n",
    "                outfile.write(np.string_(feature_table_json_min).tobytes())\n",
    "                outfile.write(np.arange(length, dtype=np.uint32).tobytes())\n",
    "                outfile.write(cartesian[::step, :].tobytes())\n",
    "                for _ in range(feature_table_padding):\n",
    "                    outfile.write(np.string_(\" \").tobytes())\n",
    "                outfile.write(np.string_(batch_table_json_min).tobytes())\n",
    "                outfile.write(value[::step].astype(np.float32).tobytes())\n",
    "                outfile.write(time[::step].astype(np.float32).tobytes())\n",
    "                outfile.write(cartographic[::step, :].tobytes())\n",
    "                for _ in range(batch_table_padding):\n",
    "                    outfile.write(np.string_(\" \").tobytes())\n",
    "                outfile.seek(0)\n",
    "\n",
    "\n",
    "    def cartographic_to_cartesian(self, start, end):\n",
    "        lon = self.lon[start:end]\n",
    "        lat = self.lat[start:end]\n",
    "        alt = self.alt[start:end]\n",
    "        size = lon.size\n",
    "\n",
    "        cartographic = np.zeros(shape=(size, 3), dtype=np.int16)\n",
    "        cartographic[:, 0] = (lon * 32767 / 180).astype(np.int16)\n",
    "        cartographic[:, 1] = (lat * 32767 / 180).astype(np.int16)\n",
    "        cartographic[:, 2] = (alt / 10).astype(np.int16)\n",
    "\n",
    "        lon = lon * to_rad\n",
    "        lat = lat * to_rad\n",
    "\n",
    "        radiiSquared = np.array([40680631590769, 40680631590769, 40408299984661.445], dtype=np.float64)\n",
    "\n",
    "        N1 = np.multiply(np.cos(lat), np.cos(lon))\n",
    "        N2 = np.multiply(np.cos(lat), np.sin(lon))\n",
    "        N3 = np.sin(lat)\n",
    "\n",
    "        magnitude = np.sqrt(np.square(N1) + np.square(N2) + np.square(N3))\n",
    "\n",
    "        N1 = N1 / magnitude\n",
    "        N2 = N2 / magnitude\n",
    "        N3 = N3 / magnitude\n",
    "\n",
    "        K1 = radiiSquared[0] * N1\n",
    "        K2 = radiiSquared[1] * N2\n",
    "        K3 = radiiSquared[2] * N3\n",
    "\n",
    "        gamma = np.sqrt(np.multiply(N1, K1) + np.multiply(N2, K2) + np.multiply(N3, K3))\n",
    "\n",
    "        K1 = K1 / gamma\n",
    "        K2 = K2 / gamma\n",
    "        K3 = K3 / gamma\n",
    "\n",
    "        N1 = np.multiply(N1, alt)\n",
    "        N2 = np.multiply(N2, alt)\n",
    "        N3 = np.multiply(N3, alt)\n",
    "\n",
    "        # x = np.multiply((N1 + K1), np.random.normal(1, .00005, N1.size))\n",
    "        # y = np.multiply((N2 + K2), np.random.normal(1, .00005, N1.size))\n",
    "        # z = np.multiply((N3 + K3), np.random.normal(1, .00005, N1.size))\n",
    "\n",
    "        x = N1 + K1\n",
    "        y = N2 + K2\n",
    "        z = N3 + K3\n",
    "\n",
    "        offset = [float(np.min(x)), float(np.min(y)), float(np.min(z))]\n",
    "\n",
    "        x = x - offset[0]\n",
    "        y = y - offset[1]\n",
    "        z = z - offset[2]\n",
    "\n",
    "        scale = [float(abs(np.max(x))), float(abs(np.max(y))), float(abs(np.max(z)))]\n",
    "\n",
    "        cartesian = np.zeros(shape=(size, 3), dtype=np.uint16)\n",
    "        cartesian[:, 0] = (x / scale[0] * 65535.0).astype(np.uint16)\n",
    "        cartesian[:, 1] = (y / scale[1] * 65535.0).astype(np.uint16)\n",
    "        cartesian[:, 2] = (z / scale[2] * 65535.0).astype(np.uint16)\n",
    "\n",
    "        region = [\n",
    "            float(np.min(lon)),\n",
    "            float(np.min(lat)),\n",
    "            float(np.max(lon)),\n",
    "            float(np.max(lat)),\n",
    "            float(np.min(alt)),\n",
    "            float(np.max(alt))\n",
    "        ]\n",
    "\n",
    "        return cartesian, offset, scale, cartographic, region\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import zarr\n",
    "import numpy as np\n",
    "import json\n",
    "import datetime as dt\n",
    "\n",
    "to_rad = np.pi / 180.0\n",
    "to_deg = 180.0 / np.pi\n",
    "\n",
    "def generate_point_cloud(variable, epoch, end, zarr_location, point_cloud_folder):\n",
    "    \"\"\"Generates json pointcloud from a given zarr file input\n",
    "\n",
    "    Args:\n",
    "        variable (_type_): _description_\n",
    "        epoch (_type_): _description_\n",
    "        end (_type_): _description_\n",
    "        zarr_location (string): source zarr file.\n",
    "        point_cloud_folder (string): destination folder for 3d tile json file.\n",
    "    \"\"\"\n",
    "\n",
    "    #out_key = f\"{os.getenv('CRS_OUTPUT_FLIGHT_PATH')}/{shortname}\"\n",
    "    #pc_out_key = f\"{output_path}/point_cloud\"\n",
    "\n",
    "    '''\n",
    "    try:\n",
    "        os.mkdir(out_key)\n",
    "    except:\n",
    "        pass\n",
    "    '''\n",
    "\n",
    "    try:\n",
    "        os.mkdir(point_cloud_folder)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # LOAD THE DATA.\n",
    "    store = zarr.DirectoryStore(zarr_location)\n",
    "    root = zarr.group(store=store)\n",
    "\n",
    "    chunk_id = root[\"chunk_id\"][:]\n",
    "    num_chunks = chunk_id.shape[0]\n",
    "    id = np.argmax(chunk_id[:, 1] > epoch) - 1\n",
    "    start_id = chunk_id[0 if id < 0 else id, 0]\n",
    "    id = num_chunks - np.argmax(chunk_id[::-1, 1] < end)\n",
    "    end_id = chunk_id[id, 0] if id < num_chunks else root[\"time\"].size - 1\n",
    "\n",
    "    root_epoch = root.attrs[\"epoch\"]\n",
    "    location = root[\"location\"][start_id:end_id]\n",
    "    lon = location[:, 0]\n",
    "    lat = location[:, 1]\n",
    "    alt = location[:, 2]\n",
    "    value = root[\"value\"][variable][start_id:end_id]\n",
    "    time = root[\"time\"][start_id:end_id]\n",
    "\n",
    "    # filter data using mask\n",
    "    epoch = epoch - root_epoch # date-time\n",
    "    end = end - root_epoch\n",
    "    mask = np.logical_and(time >= epoch, time <= end)\n",
    "    lon = lon[mask]\n",
    "    lat = lat[mask]\n",
    "    alt = alt[mask]\n",
    "    value = value[mask]\n",
    "    time = time[mask]\n",
    "\n",
    "    # Generate Pointcloud Tileset\n",
    "    point_cloud = PointCloud(point_cloud_folder, lon, lat, alt, value, time, root_epoch)\n",
    "\n",
    "    for tile in range(int(np.ceil(time.size / 530000))):\n",
    "        start_id = tile * 530000\n",
    "        end_id = np.min([start_id + 530000, time.size])\n",
    "        point_cloud.schedule_task(tile, start_id, end_id)\n",
    "\n",
    "    point_cloud.start()\n",
    "    point_cloud.join()\n",
    "\n",
    "tileset_json = {\n",
    "\t\"asset\": {\n",
    "\t\t\"version\": \"1.0\",\n",
    "\t\t\"type\": \"Airborne Radar\"\n",
    "\t},\n",
    "\t\"root\": {\n",
    "\t\t\"geometricError\": 1000000,\n",
    "\t\t\"refine\" : \"REPLACE\",\n",
    "\t\t\"boundingVolume\": {\n",
    "            \"region\": []\n",
    "        },\n",
    "        \"children\": []\n",
    "\t},\n",
    "    \"properties\": {\n",
    "        \"epoch\": \"\",\n",
    "        \"refined\": []\n",
    "    }\n",
    "}\n",
    "\n",
    "#if __name__ == '__main__':\n",
    "#    main(sys.argv[1], sys.argv[2], int(sys.argv[3]), int(sys.argv[4]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 387579\n",
      "0 0 387579\n"
     ]
    }
   ],
   "source": [
    "point_cloud_folder = f\"{zarr_folder}/point_cloud\"\n",
    "generate_point_cloud(\"ref\",  0,  1000000000000, zarr_folder, point_cloud_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2, 3)\n",
      "[[[5.68044561 9.25596638 0.71036058]\n",
      "  [0.871293   0.20218397 8.32619846]]\n",
      "\n",
      " [[7.78156751 8.70012148 9.78618342]\n",
      "  [7.99158564 4.61479362 7.80529176]]]\n",
      "[[-99.83, -99.32], [-99.79, -99.23]]\n",
      "[[42.25, 42.21], [42.63, 42.59]]\n",
      "DatetimeIndex(['2014-09-06', '2014-09-07', '2014-09-08'], dtype='datetime64[ns]', freq='D')\n",
      "2014-09-05 00:00:00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "np.random.seed(0)\n",
    "temperature = 15 + 8 * np.random.randn(2, 2, 3)\n",
    "precipitation = 10 * np.random.rand(2, 2, 3)\n",
    "lon = [[-99.83, -99.32], [-99.79, -99.23]]\n",
    "lat = [[42.25, 42.21], [42.63, 42.59]]\n",
    "time = pd.date_range(\"2014-09-06\", periods=3)\n",
    "reference_time = pd.Timestamp(\"2014-09-05\")\n",
    "print(temperature.shape)\n",
    "print(precipitation)\n",
    "print(lon)\n",
    "print(lat)\n",
    "print(time)\n",
    "print(reference_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n",
      "[1 1 1 2 2 2 3 3 3]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1,2,3])\n",
    "print(x)\n",
    "xx = np.repeat(x, 3)\n",
    "print(xx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 7, 2, 5, 8, 3, 6, 9])"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.array([1,2,3])\n",
    "b = np.array([4,5,6])\n",
    "c = np.array([7,8,9])\n",
    "res = np.column_stack((a,b,c)).reshape(-1)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "itsc-fcx-n",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
